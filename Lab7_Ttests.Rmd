
---
title: "PADP_8120_Lab7: T-tests"
author: "Tyler Scott"
date: "2015-08-15 ![Creative Commons License](images/cc-by.png)" 
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---

# Lab Topics

- t-tests

Special topic:

-data in the wild (again) for t-tests

##Goals:

After this lab you will be able to:

- Conduct 1, 2, and paired-sample t-test in R

## 
- This lab uses materials by Mine Cetinkaya-Rundel

#One and Two-sample t-tests
The R function t.test() can be used to perform both one and two sample t-tests on vectors of data. The function contains a variety of options and can be called as follows:

```{r eval=FALSE}
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), mu = 0, paired =
FALSE, var.equal = FALSE, conf.level = 0.95)
```

Here `x` is a numeric vector of data values and `y` is an optional numeric vector of data
values. If `y` is excluded, the function performs a one-sample t-test on the data contained
in `x`, if it is included it performs a two-sample t-tests using both `x` and `y`.
The option `mu` provides a number indicating the true value of the mean (or difference in
means if you are performing a two sample test) under the null hypothesis. The option
alternative is a character string specifying the alternative hypothesis, and must be one
of the following: "two.sided" (which is the default), "greater" or "less" depending on
whether the alternative hypothesis is that the mean is different than, greater than or less
than mu, respectively. For example the following call:

```{r eval=FALSE}
t.test(x, alternative = "less", mu = 10)
```

performs a one sample t-test on the data contained in x where the null hypothesis is that
=10 and the alternative is that <10. The option paired indicates whether or not you want a paired t-test (TRUE = yes and FALSE = no). If you leave this option out it defaults to FALSE.
The option var.equal is a logical variable indicating whether or not to assume the two
variances as being equal when performing a two-sample t-test. If TRUE then the pooled
variance is used to estimate the variance otherwise the Welch (or Satterthwaite)
approximation to the degrees of freedom is used. If you leave this option out it defaults
to FALSE.

Finally, the option conf.level determines the confidence level of the reported confidence
interval for in the one-sample case and 1- 2 in the two-sample case.

## One-sample t-tests

For example, an outbreak of Salmonella-related illness was attributed to ice cream produced at a
certain factory. Scientists measured the level of Salmonella in 9 randomly sampled
batches of ice cream. The levels (in MPN/g) were:

```{r eval=TRUE,echo=TRUE,results='hide',message=FALSE}
salmonella = c(0.593, 0.142, 0.329, 0.691, 0.231, 0.793, 0.519, 0.392, 0.418)
```

Is there evidence that the mean level of Salmonella in the ice cream is greater than 0.3 MPN/g? Let $\mu$ be the mean level of Salmonella in all batches of ice cream. Here the hypothesis of interest can be expressed as:

$$H_{0}: \mu = 0.3$$
$$H_{A}: \mu > 0.3$$

Hence, we will need to include the options alternative="greater", mu=0.3. Below is the
relevant R-code:

```{r eval=FALSE}
t.test(salmonella, alternative="greater", mu=0.3)
```

You should get output that shows a p-value of 0.029:

```{r echo=TRUE}
t.test(salmonella, alternative="greater", mu=0.3)
```

Hence, there is moderately strong evidence that the mean Salmonella level in the ice cream is above 0.3 MPN/g.

We can duplicate this result **without** using the `t.test` function using the `pt/qt/dt` family:

```{r eval=FALSE}
#t.obs = (obs - expected) / SD
t.obs = (mean(salmonella) - 0.3) / {sd(salmonella) / sqrt(length(salmonella))}
1 - pt(t.obs,df = length(salmonella)-1)
```

This is actually how you might prefer to compute these types of tests, because the output is easier to deal with if you are in turn feeding this value into another function or table. 

## Two-sample t-tests

6 subjects were given a drug (treatment group) and an additional 6 subjects a
placebo (control group). Their reaction time to a stimulus was measured (in ms). We
want to perform a two-sample t-test for comparing the means of the treatment and
control groups.

Let $\mu_1$ be the mean of the population taking medicine and $\mu_2$ the mean of the untreated
population. Here the hypothesis of interest can be expressed as:

$$H_{0}: \mu_1 - \mu_2 = 0$$
$$H_{A}: \mu_1 - \mu_2 != 0$$

Here we will need to include the data for the treatment group in x and the data for the
control group in y. We will also need to include the options alternative="two.sided", $\mu=0$. Finally, we need to decide whether or not the standard deviations are the same in both
groups (in a case like this where we only have six observation in each group, this really comes down to a judgment call...). Below is the relevant R-code when assuming equal standard deviation:

```{r eval=TRUE,echo=TRUE,message=FALSE,results='hide'}
Control = c(91, 87, 99, 77, 88, 91)
Treat = c(101, 110, 103, 93, 99, 104)
t.test(Control,Treat,alternative="two.sided", var.equal=TRUE)
```

Below is the relevant R-code when not assuming equal standard deviation:
```{r eval=FALSE}
t.test(Control,Treat,alternative="two.sided")
```

Note that when we **do not** assume equal variance, the function performs a Welch test instead of the standard two-sample t-test we learned in lecture. Welch's test is more reliable when the two groups: (a) have very different sample sizes; and/or (b) have unequal variances. The basic difference is that the Student's t-test computes a pooled standard deviation, whereas the Welch test uses an unpooled variance. We actually used the Welch's t-test in lecture:

$$ t_{diff} = \frac{estimate - null}{\sqrt{\frac{s^{2}_{1}}{n_{1}} + \frac{s^{2}_{2}}{n_{2}}}} $$

Here the pooled t-test and the Welsh t-test give roughly the same results (p-value =
0.00313 and 0.00339, respectively).

Again, we can duplicate this result **without** using the `t.test` function using the `pt/qt/dt` family. However, we have to take a shortcut when doing a two-sample by hand when it comes to the appropriate degress of freedom. The Welch-Satterthwaite Equation (https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation) is used to estimate the **effective** degrees of freedom within internal R functions. For our by-hand process, let's use approximate this using the 

```{r eval=FALSE}
#t.obs = (obs - expected) / SD
t.obs = ((mean(Control)-mean(Treat)) - 0) / sqrt({sd(Control)^2 / length(Control)} + {sd(Treat)^2 / length(Treat)})
2 * pt(t.obs,df = min(length(Control),length(Treat))-1)
```

Why do we need to multiply this result by 2? Recall that we are conducting a two-sided test, but `pt` only gives us the probability associated with one tail or another. Thus, we have to double the output to find the probability associated with each tail. The `t.obs` should match perfectly with the `t.test` output, but the `pt` result will be slightly different due to the shortcut we took with inputing the degrees of freedom (we used df = `r min(length(Control),length(Treat))-1` instead of the approximation 9.4797).

1. Try re-running the `pt` code above, but plug in the Welch-Satterthwaite df approximation you get as part of the `t.test` output. Do the p-values you get from the `pt` method and the `t.test` method match up now?

# Paired t-tests

There are many experimental settings where each subject in the study is in both the
treatment and control group. For example, in a matched pairs design, subjects are
matched in pairs and different treatments are given to each subject in the pair. The
outcomes are thereafter compared pair-wise. Alternatively, one can measure each
subject twice, before and after a treatment. In either of these situations we canâ€™t use
two-sample t-tests since the independence assumption is not valid. Instead we need to
use a paired t-test. This can be done using the option paired =TRUE.

For example, a study was performed to test whether cars get different mileage on premium gas than
on regular gas. Each of 10 cars was first filled with either regular or premium gas, decided by a coin toss, and the mileage for that tank was recorded. The mileage was recorded again for the same cars using the other kind of gasoline. We use a paired t-test to determine whether cars get significantly different mileage with premium gas.

```{r eval=FALSE}
reg = c(16, 20, 21, 22, 23, 22, 27, 25, 27, 28)
prem = c(19, 22, 24, 24, 25, 25, 26, 26, 28, 32)
t.test(prem,reg,alternative="two.sided", paired=TRUE)
```

Again, we can use `pt` to do this by hand (or kind-of by hand):

```{r eval=FALSE}
t.obs = mean(reg - prem) / (sd(reg-prem)/sqrt(length(reg-prem)))
2*pt(t.obs,df=9)
```

# Your turn

## North Carolina births

In 2004, the state of North Carolina released a large data set containing 
information on births recorded in this state. This data set is useful to 
researchers studying the relation between habits and practices of expectant 
mothers and the birth of their children. We will work with a random sample of 
observations from this data set.

## Exploratory analysis

Load the `nc` data set into our workspace.

```{r load-data, eval=FALSE}
download.file("http://www.openintro.org/stat/data/nc.RData", destfile = "nc.RData")
load("nc.RData")
```

We have observations on 13 different variables, some categorical and some 
numerical. The meaning of each variable is as follows.

variable         | description
---------------- | -----------
`fage`           | father's age in years.
`mage`           | mother's age in years.
`mature`         | maturity status of mother.
`weeks`          | length of pregnancy in weeks.
`premie`         | whether the birth was classified as premature (premie) or full-term.
`visits`         | number of hospital visits during pregnancy.
`marital`        | whether mother is `married` or `not married` at birth.
`gained`         | weight gained by mother during pregnancy in pounds.
`weight`         | weight of the baby at birth in pounds.
`lowbirthweight` | whether baby was classified as low birthweight (`low`) or not (`not low`).
`gender`         | gender of the baby, `female` or `male`.
`habit`          | status of the mother as a `nonsmoker` or a `smoker`.
`whitemom`       | whether mom is `white` or `not white`.

1.  What are the cases in this data set? How many cases are there in our sample?

As a first step in the analysis, we should consider summaries of the data. This
can be done using the `summary` command:

```{r summary, eval=FALSE}
summary(nc)
```

As you review the variable summaries, consider which variables are categorical 
and which are numerical. For numerical variables, are there outliers? If you 
aren't sure or want to take a closer look at the data, make a graph.

Consider the possible relationship between a mother's smoking habit and the 
weight of her baby. Plotting the data is a useful first step because it helps 
us quickly visualize trends, identify strong associations, and develop research
questions.

2.  Make a side-by-side boxplot of `habit` and `weight`. What does the plot 
highlight about the relationship between these two variables?

The box plots show how the medians of the two distributions compare, but we can
also compare the means of the distributions using the following function to 
split the `weight` variable into the `habit` groups, then take the mean of each
using the `mean` function.

```{r by-means, eval=FALSE}
by(nc$weight, nc$habit, mean)
```

3. Recall the `apply` family of functions: repeat the operation above, but use the `tapply` function

There is an observed difference, but is this difference statistically significant (i.e., is it more likely that the two "True" values are identical but we observed a difference due to random chance, or is it more likely that the two values really are different?)? In order to answer this question we will conduct a hypothesis test.

## Inference

4. Check if the conditions necessary for inference are satisfied (independence within groups, independence between groups, sample sizes both above 30, and symmetric distributions). Note that you will need to obtain sample sizes to check the conditions. You can compute the group size using the same `by` command above but replacing `mean` with `length`. You can also accomplish the same thing using the `table()` function:

```{r eval=FALSE}
table(nc$habit)
```

5  Write the hypotheses for testing if the average weights of babies born to 
smoking and non-smoking mothers are different.

6. Test this hypothesis using the `t.test` function.

7. Find the 99% confidence interval for the difference of sample means (hint: look at the `conf.level` setting within the `t.test` function)

8. Duplicate the test in item 5 using the `pt` function. 

9. Calculate a 95% confidence interval for the average length of pregnancies 
(`weeks`) and interpret it in context.

10. Calculate a new confidence interval for the same parameter at the 90% 
confidence level.

11. Conduct a hypothesis test evaluating whether the average weight gained by 
younger mothers is different than the average weight gained by mature mothers.

12. Pick a pair of numerical and categorical variables and come up with a 
research question evaluating the relationship between these variables. 
Formulate the question in a way that it can be answered using a hypothesis test
and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language.

