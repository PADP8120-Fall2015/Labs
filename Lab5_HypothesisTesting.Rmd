
---
title: "PADP_8120_Lab5: Hypothesis Testing"
author: "Tyler Scott"
date: "2015-08-03 ![Creative Commons License](images/cc-by.png)" 
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---

# Lab Topics

- Central Limit Theorem
- Data in the wild

##Goals:

After this lab you will be able to:

- Test and prove the CLT using R
- Read in, clean, and perform basic analysis of a "wild" dataset

## 

This lab is adapted from materials by:

- Andrew Poje (http://www.math.csi.cuny.edu/~poje/Teach/)

## Central Limit Theorem

Last week in lab, we explored a few sample mean properties. This week we dig in in more detail. In the lecture we just went through, we examined the Central Limit Theorem (CLT). In particular, the CLT gives us three powerful facts about the statistics of sample means.

Given random samples of size $n$ selected from some population with $mean = \mu$ and standard
deviation $\sigma$ the following relationships hold:

Fact 1: The mean of the population and the mean of the sample means are equal:

$$ \mu_{\bar{x}} $$

Fact 2: The standard deviation of the population ($\sigma$) and the standard deviation of the sample ($\sigma_{\bar{x}}$) are related by:

$$ \sigma_{\bar{x}} = \sigma \ \sqrt{n} $$

Fact 3: No matter how $x$ is distributed, the distribution of the **SAMPLE MEANS** is approximately normal as $n$ becomes larger.

# Lab Part 1

We will use R to examine these facts. First, lets create a bunch of data. Let’s consider a population of lightbulbs. The ’failure time’ of the lightbulbs has a strange distribution. Many fail immediately, but those that dont fail immediately usually last a rather long time. Can you figure out what the probability distribution of this random variable (X = time to failure) should look like?

Lets get specific and assume X is exponentially distributed. This is a standard model for
distributions which are highly skewed. To get R to take avery large, n = 100, 000 sample from an
exponential distribution, try:

```{r eval=FALSE}
x = rexp(100000,0.05)
hist(x,prob=T)
```

The population mean and the population standard deviation for this example are given by:

$$ \mu = \sigma = 20 $$

Check with R:
```{r eval=FALSE}
mean(x)
sd(x)
```

1. How close are the sample mean and sample standard deviation to the population values? 

2. Change the second input into the `rexp` function above. How does this effect the histogram and sample mean and sd values? 

3. What does this second input do?

We can repeat this to produce different samples:

```{r eval=FALSE}
xsamp = rexp(100,0.05);
mean(xsamp)

xsamp = rexp(100,0.05)
mean(xsamp)

xsamp = rexp(100,0.05)
mean(xsamp)
```

4. Write a loop that accomplishes the same thing as the 6 lines of code above.

5. Use the `replicate()` function to accomplish the same thing **without** a loop.

The Central Limit Theorem is concerned with the distribution of this sample mean. Suppose we want to look at the mean value of 500 different samples of size n = 100. We can easily create this random variable (lets call it sampmean) in R. Try this:

```{r eval=FALSE}
sampmean = replicate(500,mean(rexp(100,0.05)))
```

First, what does the distribution of sample means look like?

6. Use the `ggplot` library to make a histogram of the sample mean distribution.

7. Pretend like this is a figure for presentation/publication, and take a few minutes to make some tweaks to make it look fancy. You might want to try the `ggthemes` package for some fun style options. 

In any case, your histogram should be approximately normal. This is the main statement of the CLT While the population distribution is far from normal, the **distribution of sample means** is approximately NORMAL.

Recall that Fact 2 states that the mean ($\mu_{\bar{x}}$) and the standard deviation ($\sigma_{\bar{x}}$) of the (approximately normally distributed) sample means are related to the mean and standard deviation of the poulation by:

$$ \mu_{\bar{x}} $$

$$ \sigma_{\bar{x}} = \sigma \ \sqrt{n} $$

Let's check...

```{r eval=FALSE}
mean(sampmean)
sd(sampmean)
20/sqrt(100)
```

8. How close is the output of `sd(samplmean)` to the `20/sqrt(100)` result? Repeat the sampling process and try again. 

9. Redo the above analysis for samples of size 50, 400 and 900. Comment on the following:
(a) How do the histograms of sampmean change as the sample size is increased? Does the standard deviation increase or decrease? Is the sample mean looking ’normal’?
(b) How do the first two predictions of the central limit theorem compare to the actual data as the sample size is increased? Does µx approach µ? How about the second part of the
Central Limit Theorem?

10. Redo the analyis for a different population distribution. You may want to create data using a different binomial distribution or you may try out the R commands rexp(10000,.1) (exponential, long-tails) or rpois(1000,4) (Poisson Distribution, non-normal) or you may try something else. Whatever you chose as the population, examine what happens to various sized sample means. Check each part of the Central Limit Theorem.

11. Consider a population of lightbulbs with mean failure time $\mu = 50$ standard deviation of failure times $\sigma = 20$ days (exponential distribution). If you take a sample of 100 lightbulbs, what are the chances the mean failure time of your sample will be:

(a) Greater than 53 days? 

(b) Less than 48 days?

(c) Between 46 and 54 days?

(hint: we are going to assume a normal distribution, so use the dnorm/pnorm/qnorm/rnorm family. Also, as you're doing this, make sure to give each result the "smell test". For instance, if the mean if 50 and the distribution is normal, should the answer to (a) be less than .50 or greater than .50?)

# Lab Part 2

For the second part of our lab today, we are going to do an applied exercise that will leverage a great deal of what we have covered so far in the last several labs. Specifically, instead of using R's built-in datasets or other canned data, we are going to go out and get some data and practice some basic cleaning and data "munging" (which is a weird term that nerds and people who want to sound like experts seem to use). In any case...

To practice, let's start with some raw water quality data that I received from the Oregon Department of Ecology for a project I've been working on. You can find it in the `input` folder associated with Lab 5:

`input/Scott_OWQI_1980_2013.csv`

Most of the variables names should be somewhat intuitive, but a few might be trick:

- owqi: Oregon Water Quality Index (a composite score based upon all subindices that represent a specific metric)

- _si: suffix that denotes a subindex score

- water_yr: year of observation

Here is your assignment:

12. Water quality often varies as a function of season. In other words, we might expect that certain metrics are different in the winter (when rivers and streams have greater flow) than in the summer (when things start to dry up). Using data between 1990 and 2000, compare summer observations (months June, July, August, and September) to winter observations (months November, December, January, and February) for five different water quality metrics (`ecoli..MPN.`, `Total.PHosphorus..mg.L.`,`fecal.coliform..MPN.`,`Ammonia..mg.L.`,and `DO..mg.L.`) **AND** for the overall index (`owqi`). 

  (a) Generate an output table that compares the average summer and winter values for each of the six metrics (five subindices and overall index score) (hint: you'll need a way to parse dates - take a look at the `lubridate` package made by Hadley Wickham)

  (b) Make a box and whisker plot to present these same data

  (c) Briefly assess the quality of the data. Are there more observations from some months than others? How might this affect our results? What about the distribution of observations by year? 

  (d) What are some other potential issues that you might want other/more data to examine? 

  (e) Using functions such as `is.na`, `!is.na`, and `apply`, compute the number of missing and observed values for each variable in the dataset. 
  
  (f) Using `dplyr` and the `%>%` piping tool, find the mean value for `Nitrate.as.N.gm.L.` for each year in the dataset
  
  (g) Identify the maximum value of `Nitrate.as.N.gm.L.` observed for each year. 
  
