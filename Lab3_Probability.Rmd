
---
title: "PADP_8120_Lab3: Probability | Vectorization"
author: "Tyler Scott"
date: "2015-08-03 ![Creative Commons License](images/cc-by.png)" 
output:
  html_document:
    highlight: pygments
    theme: cerulean
---


## Lab Topics

### Subject Area

- Probability

### R Methods

- Vectorization (the `apply` family)

##Goals:

After this lab you will be able to:

- simulate data in R

- perform basic probability functions in R

and we'll cover a bonus topic: functions for vectorization in R


## 

This lab is adapted from materials by:

- Andrew Bray and Mine Cetinkaya-Rundel 



## Hot Hands

Basketball players who make several baskets in succession are described as 
having a *hot hand*. Fans and players have long believed in the hot hand 
phenomenon, which refutes the assumption that each shot is independent of the 
next. However, a 1985 paper by Gilovich, Vallone, and Tversky collected evidence
that contradicted this belief and showed that successive shots are independent 
events ([http://psych.cornell.edu/sites/default/files/Gilo.Vallone.Tversky.pdf](http://psych.cornell.edu/sites/default/files/Gilo.Vallone.Tversky.pdf)). This paper started a great controversy that continues to this day, as you can 
see by Googling *hot hand basketball*. We do not expect to resolve this controversy today. However, in this lab we'll apply one approach to answering questions like this. 



## Getting Started

Our investigation will focus on the performance of one player: Kobe Bryant of 
the Los Angeles Lakers. His performance against the Orlando Magic in the 2009 
NBA finals earned him the title *Most Valuable Player* and many spectators 
commented on how he appeared to show a hot hand. Let's load some data from those
games and look at the first several rows.

```{r load-data, eval=FALSE}
download.file("http://www.openintro.org/stat/data/kobe.RData", destfile = "kobe.RData")
load("input/kobe.RData")
head(kobe)
```

In this data frame, every row records a shot taken by Kobe Bryant. If he hit the
shot (made a basket), a hit, `H`, is recorded in the column named `basket`, 
otherwise a miss, `M`, is recorded.

Just looking at the string of hits and misses, it can be difficult to gauge 
whether or not it seems like Kobe was shooting with a hot hand. One way we can 
approach this is by considering the belief that hot hand shooters tend to go on 
shooting streaks. For this lab, we define the length of a shooting streak to be 
the *number of consecutive baskets made until a miss occurs*.

For example, in Game 1 Kobe had the following sequence of hits and misses from 
his nine shot attempts in the first quarter:

\[ \textrm{H M | M | H H M | M | M | M} \]

To verify this use the following command:

```{r first9, eval=FALSE}
kobe$basket[1:9]
```

Within the nine shot attempts, there are six streaks, which are separated by a 
"|" above. Their lengths are one, zero, two, zero, zero, zero (in order of 
occurrence).

1.  What does a streak length of 1 mean, i.e. how many hits and misses are in a 
    streak of 1? What about a streak length of 0?

The custom function `calc_streak`, which was loaded in with the data, may be 
used to calculate the lengths of all shooting streaks and then look at the 
distribution.

```{r calc-streak-kobe, eval=FALSE}
kobe_streak <- calc_streak(kobe$basket)
barplot(table(kobe_streak))
```

Note that instead of making a histogram, we chose to make a bar plot from a 
table of the streak data. A bar plot is preferable here since our variable is 
discrete -- counts -- instead of continuous.

2.  Describe the distribution of Kobe's streak lengths from the 2009 NBA finals. 
    What was his typical streak length? How long was his longest streak of baskets?

## Compared to What?

We've shown that Kobe had some long shooting streaks, but are they long enough 
to support the belief that he had hot hands? What can we compare them to?

To answer these questions, let's return to the idea of *independence*. Two 
processes are independent if the outcome of one process doesn't effect the outcome 
of the second. If each shot that a player takes is an independent process, 
having made or missed your first shot will not affect the probability that you
will make or miss your second shot.

A shooter with a hot hand will have shots that are *not* independent of one 
another. Specifically, if the shooter makes his first shot, the hot hand model 
says he will have a *higher* probability of making his second shot.

Let's suppose for a moment that the hot hand model is valid for Kobe. During his
career, the percentage of time Kobe makes a basket (i.e. his shooting 
percentage) is about 45%, or in probability notation,

\[ P(\textrm{shot 1 = H}) = 0.45 \]

If he makes the first shot and has a hot hand (*not* independent shots), then 
the probability that he makes his second shot would go up to, let's say, 60%,

\[ P(\textrm{shot 2 = H} \, | \, \textrm{shot 1 = H}) = 0.60 \]

As a result of these increased probabilites, you'd expect Kobe to have longer 
streaks. Compare this to the skeptical perspective where Kobe does *not* have a
hot hand, where each shot is independent of the next. If he hit his first shot,
the probability that he makes the second is still 0.45.

\[ P(\textrm{shot 2 = H} \, | \, \textrm{shot 1 = H}) = 0.45 \]

In other words, making the first shot did nothing to effect the probability that
he'd make his second shot. If Kobe's shots are independent, then he'd have the 
same probability of hitting every shot regardless of his past shots: 45%.

Now that we've phrased the situation in terms of independent shots, let's return
to the question: how do we tell if Kobe's shooting streaks are long enough to 
indicate that he has hot hands? We can compare his streak lengths to someone
without hot hands: an independent shooter. 

## Simulations in R

While we don't have any data from a shooter we know to have independent shots, 
that sort of data is very easy to simulate in R. In a simulation, you set the 
ground rules of a random process and then the computer uses random numbers to 
generate an outcome that adheres to those rules. As a simple example, you can
simulate flipping a fair coin with the following.

```{r head-tail, eval=FALSE}
outcomes <- c("heads", "tails")
sample(outcomes, size = 1, replace = TRUE)
```

The vector `outcomes` can be thought of as a hat with two slips of paper in it: 
one slip says `heads` and the other says `tails`. The function `sample` draws 
one slip from the hat and tells us if it was a head or a tail. 

Run the second command listed above several times. Just like when flipping a 
coin, sometimes you'll get a heads, sometimes you'll get a tails, but in the 
long run, you'd expect to get roughly equal numbers of each.

If you wanted to simulate flipping a fair coin 100 times, you could either run 
the function 100 times or, more simply, adjust the `size` argument, which 
governs how many samples to draw (the `replace = TRUE` argument indicates we put
the slip of paper back in the hat before drawing again). Save the resulting 
vector of heads and tails in a new object called `sim_fair_coin`.

```{r sim-fair-coin, eval=FALSE}
sim_fair_coin <- sample(outcomes, size = 100, replace = TRUE)
```

To view the results of this simulation, type the name of the object and then use
`table` to count up the number of heads and tails.

```{r table-sim-fair-coin, eval=FALSE}
sim_fair_coin
table(sim_fair_coin)
```

Since there are only two elements in `outcomes`, the probability that we "flip" 
a coin and it lands heads is 0.5. Say we're trying to simulate an unfair coin 
that we know only lands heads 20% of the time. We can adjust for this by adding 
an argument called `prob`, which provides a vector of two probability weights.

```{r sim-unfair-coin, eval=FALSE}
sim_unfair_coin <- sample(outcomes, size = 100, replace = TRUE, prob = c(0.2, 0.8))
```

`prob=c(0.2, 0.8)` indicates that for the two elements in the `outcomes` vector,
we want to select the first one, `heads`, with probability 0.2 and the second 
one, `tails` with probability 0.8. Another way of thinking about this is to 
think of the outcome space as a bag of 10 chips, where 2 chips are labeled 
"head" and 8 chips "tail". Therefore at each draw, the probability of drawing a 
chip that says "head"" is 20%, and "tail" is 80%.

3.  In your simulation of flipping the unfair coin 100 times, how many flips 
    came up heads?

In a sense, we've shrunken the size of the slip of paper that says "heads", 
making it less likely to be drawn and we've increased the size of the slip of 
paper saying "tails", making it more likely to be drawn. When we simulated the 
fair coin, both slips of paper were the same size. This happens by default if 
you don't provide a `prob` argument; all elements in the `outcomes` vector have 
an equal probability of being drawn.

If you want to learn more about `sample` or any other function, recall that you 
can always check out its help file.

```{r help-sample, eval=FALSE,tidy = FALSE}
?sample
```

## Simulating the Independent Shooter

Simulating a basketball player who has independent shots uses the same mechanism 
that we use to simulate a coin flip. To simulate a single shot from an 
independent shooter with a shooting percentage of 50% we type,

```{r sim-basket, eval=FALSE}
outcomes <- c("H", "M")
sim_basket <- sample(outcomes, size = 1, replace = TRUE)
```

To make a valid comparison between Kobe and our simulated independent shooter, 
we need to align both their shooting percentage and the number of attempted shots.

4.  What change needs to be made to the `sample` function so that it reflects a 
    shooting percentage of 45%? Make this adjustment, then run a simulation to 
    sample 133 shots. Assign the output of this simulation to a new object called
    `sim_basket`.

Note that we've named the new vector `sim_basket`, the same name that we gave to
the previous vector reflecting a shooting percentage of 50%. In this situation, 
R overwrites the old object with the new one, so always make sure that you don't
need the information in an old vector before reassigning its name.

With the results of the simulation saved as `sim_basket`, we have the data 
necessary to compare Kobe to our independent shooter. We can look at Kobe's data 
alongside our simulated data.

```{r compare-basket, eval=FALSE}
kobe$basket
sim_basket
```

Both data sets represent the results of 133 shot attempts, each with the same 
shooting percentage of 45%. We know that our simulated data is from a shooter 
that has independent shots. That is, we know the simulated shooter does not have
a hot hand.

### Comparing Kobe Bryant to the Independent Shooter

Using `calc_streak`, compute the streak lengths of `sim_basket`.

5. Describe the distribution of streak lengths. What is the typical streak 
    length for this simulated independent shooter with a 45% shooting percentage?
    How long is the player's longest streak of baskets in 133 shots?

6. If you were to run the simulation of the independent shooter a second time, 
    how would you expect its streak distribution to compare to the distribution 
    from the question above? Exactly the same? Somewhat similar? Totally 
    different? Explain your reasoning.

7. How does Kobe Bryant's distribution of streak lengths compare to the 
    distribution of streak lengths for the simulated shooter? Using this 
    comparison, do you have evidence that the hot hand model fits Kobe's 
    shooting patterns? Explain.


## Special Section: Vector-wise operations and loops


## apply

`apply` is used to a evaluate a function (often an anonymous one) over the margins of an array.

- It is most often used to apply a function to the rows or columns of a matrix

- It can be used with general arrays, e.g. taking the average of an array of matrices 

- It is not really faster than writing a loop, but it works in one line!

---

## apply

```r
> str(apply)
function (X, MARGIN, FUN, ...)
```

- `X` is an array
- `MARGIN` is an integer vector indicating which margins should be “retained”. 
- `FUN` is a function to be applied
- ... is for other arguments to be passed to `FUN`

---

## apply

```r
> x <- matrix(rnorm(200), 20, 10)
> apply(x, 2, mean)
 [1]  0.04868268  0.35743615 -0.09104379
 [4] -0.05381370 -0.16552070 -0.18192493
 [7]  0.10285727  0.36519270  0.14898850
[10]  0.26767260

> apply(x, 1, sum)
 [1] -1.94843314  2.60601195  1.51772391
 [4] -2.80386816  3.73728682 -1.69371360
 [7]  0.02359932  3.91874808 -2.39902859
[10]  0.48685925 -1.77576824 -3.34016277
[13]  4.04101009  0.46515429  1.83687755
[16]  4.36744690  2.21993789  2.60983764
[19] -1.48607630  3.58709251
```

---

## col/row sums and means

For sums and means of matrix dimensions, we have some shortcuts.

- `rowSums` = `apply(x, 1, sum)`
- `rowMeans` = `apply(x, 1, mean)`
- `colSums` = `apply(x, 2, sum)`
- `colMeans` = `apply(x, 2, mean)`

The shortcut functions are _much_ faster, but you won’t notice unless you’re using a large matrix.

---

## Other Ways to Apply

Quantiles of the rows of a matrix.

```r
> x <- matrix(rnorm(200), 20, 10)
> apply(x, 1, quantile, probs = c(0.25, 0.75))
          [,1]        [,2]       [,3]        [,4]
25% -0.3304284 -0.99812467 -0.9186279 -0.49711686
75%  0.9258157  0.07065724  0.3050407 -0.06585436
           [,5]       [,6]      [,7]       [,8]
25% -0.05999553 -0.6588380 -0.653250 0.01749997
75%  0.52928743  0.3727449  1.255089 0.72318419
          [,9]      [,10]      [,11]      [,12]
25% -1.2467955 -0.8378429 -1.0488430 -0.7054902
75%  0.3352377  0.7297176  0.3113434  0.4581150
         [,13]      [,14]      [,15]      [,16]
25% -0.1895108 -0.5729407 -0.5968578 -0.9517069
75%  0.5326299  0.5064267  0.4933852  0.8868922
         [,17]      [,18]      [,19]     [,20]
```

---

## apply

Average matrix in an array

```r
> a <- array(rnorm(2 * 2 * 10), c(2, 2, 10))
> apply(a, c(1, 2), mean)
           [,1]        [,2]
[1,] -0.2353245 -0.03980211
[2,] -0.3339748  0.04364908

> rowMeans(a, dims = 2)
           [,1]        [,2]
[1,] -0.2353245 -0.03980211
[2,] -0.3339748  0.04364908
```


## Looping on the Command Line

There are some functions which implement `apply` style looping to make life easier.

- `lapply`: Loop over a list and evaluate a function on each element 

- `sapply`: Same as `lapply` but try to simplify the result

- `apply`: Apply a function over the margins of an array

- `tapply`: Apply a function over subsets of a vector

- `mapply`: Multivariate version of `lapply`

An auxiliary function `split` is also useful, particularly in conjunction with `lapply`.

---

## lapply

`lapply` takes three arguments: (1) a list `X`; (2) a function (or the
name of a function) `FUN`; (3) other arguments via its
... argument. If `X` is not a list, it will be coerced to a list using
`as.list`.

```{r}
lapply
```

The actual looping is done internally in C code.

---

## lapply

`lapply` always returns a list, regardless of the class of the input.

```{r}
x <- list(a = 1:5, b = rnorm(10))
lapply(x, mean)
```

---

## lapply

```{r}
x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
lapply(x, mean)
```

---

## lapply

```r
> x <- 1:4
> lapply(x, runif)
[[1]]
[1] 0.2675082

[[2]]
[1] 0.2186453 0.5167968

[[3]]
[1] 0.2689506 0.1811683 0.5185761

[[4]]
[1] 0.5627829 0.1291569 0.2563676 0.7179353
```

---

## lapply

```r
> x <- 1:4
> lapply(x, runif, min = 0, max = 10)
[[1]]
[1] 3.302142

[[2]]
[1] 6.848960 7.195282

[[3]]
[1] 3.5031416 0.8465707 9.7421014

[[4]]
[1] 1.195114 3.594027 2.930794 2.766946
```

---

## lapply

`lapply` and friends make heavy use of _anonymous_ functions.

```r
> x <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2)) 
> x
$a
     [,1] [,2]
[1,]    1    3
[2,]    2    4

$b
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
```

---

## lapply

An anonymous function for extracting the first column of each matrix.

```r
> lapply(x, function(elt) elt[,1])
$a
[1] 1 2

$b
[1] 1 2 3
```

---

## sapply

`sapply` will try to simplify the result of `lapply` if possible.

- If the result is a list where every element is length 1, then a vector is returned

- If the result is a list where every element is a vector of the same length (> 1), a matrix is returned.

- If it can’t figure things out, a list is returned

---

## sapply

```r
> x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
> lapply(x, mean)
$a
[1] 2.5

$b
[1] 0.06082667

$c
[1] 1.467083

$d
[1] 5.074749
```

---

## sapply

```r
> sapply(x, mean) 
         a          b          c          d
2.50000000 0.06082667 1.46708277 5.07474950

> mean(x)
[1] NA
Warning message:
In mean.default(x) : argument is not numeric or logical: returning NA
```


## tapply

`tapply` is used to apply a function over subsets of a vector. I don’t know why it’s called `tapply`.

```r
> str(tapply)
function (X, INDEX, FUN = NULL, ..., simplify = TRUE)
```

- `X` is a vector
- `INDEX` is a factor or a list of factors (or else they are coerced to factors) 
- `FUN` is a function to be applied
- ... contains other arguments to be passed `FUN`
- `simplify`, should we simplify the result?

---

## tapply

Take group means.

```r
> x <- c(rnorm(10), runif(10), rnorm(10, 1))
> f <- gl(3, 10)
> f
 [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3
[24] 3 3 3 3 3 3 3
Levels: 1 2 3
> tapply(x, f, mean)
        1         2         3 
0.1144464 0.5163468 1.2463678
```

---

## tapply

Take group means without simplification.

```r
> tapply(x, f, mean, simplify = FALSE)
$‘1‘
[1] 0.1144464

$‘2‘
[1] 0.5163468

$‘3‘
[1] 1.246368
```

---

## tapply

Find group ranges.

```r
> tapply(x, f, range)
$‘1‘
[1] -1.097309  2.694970

$‘2‘
[1] 0.09479023 0.79107293

$‘3‘
[1] 0.4717443 2.5887025
```

---

##
 
To pratice this, let's look at some data (obtained from the Open Intro website) from the CDC.

```{r}
library(ggplot2)
source("http://www.openintro.org/stat/data/cdc.R")
```

```{r}
## ----look-at-data--------------------------------------------------------
# Get an idea of data
str(cdc)
head(cdc)
names(cdc)
summary(cdc)
```

8. Calculate the average age of males and females using `subset` and `mean`

9. Now, repeat this calculation but use the `tapply` function. 

10. Use `tapply` to calculate the minimum and maximum ages for males and females.

Just for kicks, let's make a list of different variables:

```{r eval=FALSE}
varlist = list(cdc$height,cdc$weight,cdc$age)
```

11. Use `lapply` to calculate the mean and sd of each variable in the `varlist` object. 

12. Perform the same operations using `sapply`. How does the output differ? 

13. When might you prefer to use `sapply` and when might you prefer to use `lapply`?



