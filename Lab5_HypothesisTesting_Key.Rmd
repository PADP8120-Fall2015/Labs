
---
title: "PADP_8120_Lab5: Hypothesis Testing"
author: "Tyler Scott"
date: "2015-08-03 ![Creative Commons License](images/cc-by.png)" 
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---

# Lab Topics

- Central Limit Theorem
- Data in the wild

##Goals:

After this lab you will be able to:

- Test and prove the CLT using R
- Read in, clean, and perform basic analysis of a "wild" dataset

## 

This lab is adapted from materials by:

- Andrew Poje (http://www.math.csi.cuny.edu/~poje/Teach/)

## Central Limit Theorem

Last week in lab, we explored a few sample mean properties. This week we dig in in more detail. In the lecture we just went through, we examined the Central Limit Theorem (CLT). In particular, the CLT gives us three powerful facts about the statistics of sample means.

Given random samples of size $n$ selected from some population with $mean = \mu$ and standard
deviation $\sigma$ the following relationships hold:

Fact 1: The mean of the population and the mean of the sample means are equal:

$$ \mu_{\bar{x}} $$

Fact 2: The standard deviation of the population ($\sigma$) and the standard deviation of the sample ($\sigma_{\bar{x}}$) are related by:

$$ \sigma_{\bar{x}} = \sigma \ \sqrt{n} $$

Fact 3: No matter how $x$ is distributed, the distribution of the **SAMPLE MEANS** is approximately normal as $n$ becomes larger.

# Lab Part 1

We will use R to examine these facts. First, lets create a bunch of data. Let’s consider a population of lightbulbs. The ’failure time’ of the lightbulbs has a strange distribution. Many fail immediately, but those that dont fail immediately usually last a rather long time. Can you figure out what the probability distribution of this random variable (X = time to failure) should look like?

Lets get specific and assume X is exponentially distributed. This is a standard model for
distributions which are highly skewed. To get R to take avery large, n = 100, 000 sample from an
exponential distribution, try:

```{r eval=TRUE}
x = rexp(100000,0.05)
hist(x,prob=T)
```

The population mean and the population standard deviation for this example are given by:

$$ \mu = \sigma = 20 $$

Check with R:
```{r eval=TRUE}
mean(x)
sd(x)
```

1. How close are the sample mean and sample standard deviation to the population values? 

It looks like the sample values are pretty close to the population values. Since we're taking a new sample each time the document is compiled (and the code is run), the extent to which things actually differ changes each time, but since we take 10,000 draws the sample values should pretty much approximate the population values. 

2. Change the second input into the `rexp` function above. How does this effect the histogram and sample mean and sd values? 

The second input is the rate parameter, which effects the shape of the exponential curve (it is called the "rate" because it is the decay rate for the function). As the rate parameter increases, the curve drops more steeply (i.e, things decay faster), and as the rate parameter decreases, the curve descends more slowly. 

```{r eval=TRUE}
x = rexp(100000,0.05)
hist(x,prob=T)
```

```{r eval=TRUE}
x = rexp(100000,1)
hist(x,prob=T)
```

```{r eval=TRUE}
x = rexp(100000,0.00001)
hist(x,prob=T)
```

3. What does this second input do?

As discussed above, the second input determines the shape of the exponential distribution.

We can repeat this to produce different samples:

```{r eval=FALSE}
xsamp = rexp(100,0.05);
mean(xsamp)

xsamp = rexp(100,0.05)
mean(xsamp)

xsamp = rexp(100,0.05)
mean(xsamp)
```

4. Write a loop that accomplishes the same thing as the 6 lines of code above.

```{r eval=TRUE}
sample.mean.vector = rep(NA,3)
for (i in 1:3)
{
  sample.mean.vector[i] <- mean(rexp(100,0.05))
}
sample.mean.vector
```

5. Use the `replicate()` function to accomplish the same thing **without** a loop.

```{r eval=TRUE}
sample.mean.vector <- replicate(3,mean(rexp(100,0.05)))
sample.mean.vector
```


The Central Limit Theorem is concerned with the distribution of this sample mean. Suppose we want to look at the mean value of 500 different samples of size n = 100. We can easily create this random variable (lets call it sampmean) in R. Try this:

```{r eval=TRUE}
sampmean = replicate(500,mean(rexp(100,0.05)))
```

First, what does the distribution of sample means look like?

6. Use the `ggplot2` library to make a histogram of the sample mean distribution.

```{r message=FALSE}
library(ggplot2)
ggplot() + geom_histogram(aes(x=sampmean),stat = 'density')
#note that setting stat='density' makes a probability density function out of the basic histogram
```

7. Pretend like this is a figure for presentation/publication, and take a few minutes to make some tweaks to make it look fancy. You might want to try the `ggthemes` package for some fun style options. 

```{r}
library(ggthemes)
ggplot() + geom_histogram(aes(x=sampmean),fill=NA, color="black") + theme_bw()  +
scale_x_continuous('Sample Mean',expand=c(0,0)) +
  ylab('Frequency') + theme_tufte()
```


In any case, your histogram should be approximately normal. This is the main statement of the CLT While the population distribution is far from normal, the **distribution of sample means** is approximately NORMAL.

Recall that Fact 2 states that the mean ($\mu_{\bar{x}}$) and the standard deviation ($\sigma_{\bar{x}}$) of the (approximately normally distributed) sample means are related to the mean and standard deviation of the population by:

$$ \mu_{\bar{x}} $$

$$ \sigma_{\bar{x}} = \sigma \ \sqrt{n} $$

Let's check...

```{r eval=TRUE}
sampmean = replicate(500,mean(rexp(100,0.05)))
mean(sampmean)
sd(sampmean)
20/sqrt(100)
```

8. How close is the output of `sd(samplmean)` to the `20/sqrt(100)` result? Repeat the sampling process and try again. 

```{r eval=TRUE}
mean(sampmean)
sd(sampmean)
20/sqrt(100)
```

9. Redo the above analysis for samples of size 50, 400 and 900. Comment on the following:

(a) How do the histograms of sampmean change as the sample size is increased? Does the standard deviation increase or decrease? Is the sample mean looking ’normal’?

By fixing the x-axis limits, it is easy to see how the four distributions differ. As in the prior instance above where we performed a similar analysis, the larger sample size creates a more nearly normal distribution that is more centered around the population mean (20):

```{r eval=TRUE}
sampmean50 = replicate(50,mean(rexp(100,0.05)))
sampmean400 = replicate(400,mean(rexp(100,0.05)))
sampmean500 = replicate(500,mean(rexp(100,0.05)))
sampmean900 = replicate(900,mean(rexp(100,0.05)))
par(mfrow=c(2,2))
hist(sampmean50,main='',ylab='',xlim=c(10,40))
hist(sampmean400,main='',ylab='',xlim=c(10,40))
hist(sampmean500,main='',ylab='',xlim=c(10,40))
hist(sampmean900,main='',ylab='',xlim=c(10,40))
```

(b) How do the first two predictions of the central limit theorem compare to the actual data as the sample size is increased? Does $\mu_x$ approach $\mu$? How about the second part of the
Central Limit Theorem?

The first prediction is simply that the mean *of the sample means* ($\mu_X$) is equal to the population mean, $\mu$. 
```{r}
mean(sampmean50)
mean(sampmean400)
mean(sampmean500)
mean(sampmean900)
```

Note that here, the sample size doesn't make all that much difference. On average, we would expect the larger sample set to be closer to the population mean, but taking fifty samples of 100 draws vs. 900 samples of 100 draws doesn't seem to affect the estimate all that much. 

The second prediction is that the `sd(sampmean)` should approximately equal `$\sigma/sqrt(n)$`, or `20/sqrt(100)` in our case. Again, all four samples get pretty close, with the larger samples performing a good bit better. In fact, sample size seems slightly more important here than with the means above. 
```{r}
20/sqrt(100)
sd(sampmean50)
sd(sampmean400)
sd(sampmean500)
sd(sampmean900)
```


10. Redo the analyis for a different population distribution. You may want to create data using a different binomial distribution or you may try out the R commands rexp(10000,.1) (exponential, long-tails) or rpois(1000,4) (Poisson Distribution, non-normal) or you may try something else. Whatever you chose as the population, examine what happens to various sized sample means. Check each part of the Central Limit Theorem.

```{r eval=TRUE}
#rbinom(#ofdraws,#numberoftrials,p(success))
sampmean50 = replicate(50,mean(rbinom(100,1,.5)))
sampmean400 = replicate(400,mean(rbinom(100,1,.5)))
sampmean500 = replicate(500,mean(rbinom(100,1,.5)))
sampmean900 = replicate(900,mean(rbinom(100,1,.5)))
par(mfrow=c(2,2))
hist(sampmean50,main='',ylab='')
hist(sampmean400,main='',ylab='')
hist(sampmean500,main='',ylab='')
hist(sampmean900,main='',ylab='')
```

Since p(success) = .5, and we took 100 samples of 1 draw each, the average should be about 0.5 (what we get by averaging all of the 0's and 1's):

```{r}
mean(sampmean50)
mean(sampmean400)
mean(sampmean500)
mean(sampmean900)
```

The standard deviation of a binomial distribution is given by $\sigma = \sqrt{np(1-p)}$, which in this case is: $\sigma = \sqrt{1*0.5(1-0.5)}$. Looking at the results below, it looks as if the CLT will live on another day!
```{r}
sqrt(1*0.5*(1-0.5))/sqrt(100)
sd(sampmean50)
sd(sampmean400)
sd(sampmean500)
sd(sampmean900)
```

11. Consider a population of lightbulbs with mean failure time $\mu = 50$ standard deviation of failure times $\sigma = 20$ days. If you take a sample of 100 lightbulbs, what are the chances the mean failure time of your sample will be:

(hint: Whereas before we used an exponential distribution for lightbulbs, here we are going to assume a normal distribution, so use the dnorm/pnorm/qnorm/rnorm family. Also, as you're doing this, make sure to give each result the "smell test". For instance, if the mean if 50 and the distribution is normal, should the answer to (a) be less than .50 or greater than .50?)

(a) Greater than 53 days? 

```{r}
pnorm(53,mean=50,sd = 20,lower.tail=FALSE)
```

The probability that the mean will be greater than 53 ($p(\bar{X})>53)$) equals 0.44.

(b) Less than 48 days?

```{r}
pnorm(48,mean=50,sd = 20,lower.tail=TRUE)
```

The probability that the mean will be less than 48 ($p(\bar{X})<48)$) equals 0.46.


(c) Between 46 and 54 days?

The easiest way to do this and keep everything straight is to start 1 one and subtract the tails you want to exclude.

```{r}
1 - pnorm(46,mean=50,sd = 20,lower.tail=TRUE) - pnorm(54,mean=50,sd = 20,lower.tail=FALSE)
```

However, since the normal distribution is symmetric and $\mu - |X| = 4$ in both cases, we could also just use:

```{r}
1 - 2 * pnorm(46,mean=50,sd = 20,lower.tail=TRUE)
```

In any case, that might seem like a lot of weight in the tails, but we can plot this distribution to see why. As it turns out, the standard deviation is pretty high, and so this is a very fat tailed distribution. 

```{r}
par(mfrow=c(1,1))
testsample = rnorm(10000,mean=50,sd=20)
hist(testsample,breaks=100)
abline(v=50+c(-4,4),col='red',lty=2)
```


# Lab Part 2

For the second part of our lab today, we are going to do an applied exercise that will leverage a great deal of what we have covered so far in the last several labs. Specifically, instead of using R's built-in datasets or other canned data, we are going to go out and get some data and practice some basic cleaning and data "munging" (which is a weird term that nerds and people who want to sound like experts seem to use). In any case...

To practice, let's start with some raw water quality data that I received from the Oregon Department of Ecology for a project I've been working on. You can find it in the `input` folder associated with Lab 5:

`input/Scott_OWQI_1980_2013.csv`

Most of the variables names should be somewhat intuitive, but a few might be trick:

- owqi: Oregon Water Quality Index (a composite score based upon all subindices that represent a specific metric)

- _si: suffix that denotes a subindex score

- water_yr: year of observation

Here is your assignment:

12. Water quality often varies as a function of season. In other words, we might expect that certain metrics are different in the winter (when rivers and streams have greater flow) than in the summer (when things start to dry up). Using data between 1990 and 2000, compare summer observations (months June, July, August, and September) to winter observations (months November, December, January, and February) for five different water quality metrics (`ecoli..MPN.`, `Total.PHosphorus..mg.L.`,`fecal.coliform..MPN.`,`Ammonia..mg.L.`,and `DO..mg.L.`) **AND** for the overall index (`owqi`). 

  (a) Generate an output table that compares the average summer and winter values for each of the six metrics (five subindices and overall index score) (hint: you'll need a way to parse dates - take a look at the `lubridate` package made by Hadley Wickham)

```{r message =FALSE}
wq.df <- read.csv("input/Scott_OWQI_1980_2013.csv",header=T,skip=1)
library(lubridate)
library(magrittr)
library(dplyr)
wq.df <- wq.df %>% mutate(Date = as.Date(wq.df$Date, "%m/%d/%y"),Month = month(Date))
wq.df$Season = NA 
wq.df$Season <- ifelse(wq.df$Month %in% 6:9,'Summer',wq.df$Season)
wq.df$Season <- ifelse(wq.df$Month %in% c(11,12,1,2),'Winter',wq.df$Season)
submetrics = c('ecoli..MPN.', 'Total.PHosphorus..mg.L.','fecal.coliform..MPN.','Ammonia..mg.L.', 'DO..mg.L.')

wq.df %>% filter(!is.na(Season)) %>% group_by(Season) %>% summarise_each(funs(mean(.,na.rm=T)),ecoli..MPN., Total.Phosphorus..mg.L.,fecal.coliform..MPN.,Ammonia..mg.L., DO..mg.L.,owqi)
```

  (b) Make a box and whisker plot to present these same data

```{r}
library(tidyr)
wq.long.file = wq.df %>% select(Season,ecoli..MPN., Total.Phosphorus..mg.L.,fecal.coliform..MPN.,Ammonia..mg.L., DO..mg.L.,owqi) %>%
 tidyr::gather(Metric,Value,-Season) %>% filter(!is.na(Season),!is.na(Value)) %>%
  mutate(Value = as.numeric(Value))

library(ggthemes)
p = ggplot(data=wq.long.file) 
p + geom_boxplot(aes(y=Value,x=Season)) + facet_wrap(~Metric,scales = 'free_y') + theme_tufte()
```


  (c) Briefly assess the quality of the data. Are there more observations from some months than others? How might this affect our results? What about the distribution of observations by year? 

The data appear to be relatively even by month:
```{r}
table(wq.df$Month)
```

and by year things actually don't look too bad either. Nothing is completely balanced, but overall this looks like a failry even year-to-year comparison. 
```{r}
table(year(wq.df$Date))
```

The thing we would want to be careful of is if there are certain key differences between years (or months) with more and less observations. For instance, if one year was a heavy precipitation year and was sample much more than normal as well, this could throw off the estimation. Of course, we also need to keep in mind that as we add more years and more observations, things even out, so even small issues like that likely don't pose an issue here, particularly if we are able to control for other key variables such as precipation in the subsequent analysis. 

  (d) What are some other potential issues that you might want other/more data to examine? 

You could seek any number of things here, but one obvious one is that the location of these samples matters a lot. Depending on your purposes, you would want to know where these data were taken from to so you could control for things like being from the same river, elevation, land-use, etc. 


  (e) Using functions such as `is.na`, `!is.na`, and `apply`, compute the number of missing and observed values for each variable in the dataset. 

DO NOT do this "by hand". Instead, we can automate:
```{r}
#is.na will produce true/false of every obs in wq.df, then, I use apply with margin=2 to sum by column
apply(is.na(wq.df),2,sum)

#To produce #obs in instead of #missing, just use apply(!is.na(wq.df),2,sum) instead
```

Some values have a TON of NA value. For instance, about half of all `ecoli..MPN.` observations are missing. On the other hand, we have all `Total.Phosphorus..mg.L.` observations.

  (f) Using `dplyr` and the `%>%` piping tool, find the mean value for `Nitrate.as.N.gm.L.` for each year in the dataset
  
```{r}
nitrate.by.year <- wq.df %>% dplyr::rename(Nitrate = Nitrate.as.N.mg.L.) %>% 
  mutate(Year = year(Date),Nitrate = as.numeric(Nitrate)) %>%
  group_by(Year) %>% summarise(mean(Nitrate,na.rm=T))
nitrate.by.year
```
  
  (g) Identify the maximum value of `Nitrate.as.N.gm.L.` observed for each year. 
  
This is basically the same operation, but we substitute `max` for `mean`:
```{r}
max.nitrate.by.year <- wq.df %>% dplyr::rename(Nitrate = Nitrate.as.N.mg.L.) %>% 
  mutate(Year = year(Date),Nitrate = as.numeric(Nitrate)) %>%
  group_by(Year) %>% summarise(max(Nitrate,na.rm=T))
max.nitrate.by.year
```